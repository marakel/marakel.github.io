---
title: "Predicting House Prices"
date: 2019-10-11
tags: [python, data analysis, regression]

---

I am using a dataset of house prices that includes about 80 features. My main goal is to predict the "SalePrice" values using a combination of the other features. I am using three main functions: "trans_features" is used for transforming features which includes dropping unecessary columns. The second function is "select_features" which is mianly using a correlation coefficient threshold to select the final features to fit a model. The third function "train_and_test" is for splitting the data into training and test data, choosing one of two cross validation types, fitting the model, and then outputting the final predicted "SalePrice" values. 
The Ames Housing dataset I used can be found here: https://www.kaggle.com/prevek18/ames-housing-dataset.



```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn import linear_model

#Loading in csv file 
house_df = pd.read_csv("C:\\Users\\Matt\\Desktop\\AmesHousing.tsv", delimiter = "\t")
```




```python
#This functions is used to transform features. I am mainly using it to drop unnecessary columns
def trans_features(house_df):
    ##All columns: Drop any with 5% or more missing values for now.
    numerical_missing = house_df.isnull().sum()
    drop_missing_cols_1 = numerical_missing[(numerical_missing > len(house_df)/20)].sort_values()
    house_df = house_df.drop(drop_missing_cols_1.index, axis=1)
    
    ##For text columns: Drop any with 1 or more missing values for now
    text_mv_counts = house_df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)
    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]
    house_df = house_df.drop(drop_missing_cols_2.index, axis=1)
    
    ##For numerical columns: for cols missing info, fill in with most common value in that column
    numerical_missing = house_df.select_dtypes(include=['int', 'float']).isnull().sum()
    fixable_numeric_cols = numerical_missing[(numerical_missing < len(house_df)/20) & (numerical_missing > 0)].sort_values()
    replacement_values_dict = house_df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]
    house_df = house_df.fillna(replacement_values_dict)
    years_sold = house_df['Yr Sold'] - house_df['Year Built']
    years_since_remod = house_df['Yr Sold'] - house_df['Year Remod/Add']
    
    #Create new columns
    house_df['Years Before Sale'] = years_sold
    house_df['Years Since Remod'] = years_since_remod
    
    #Dropping unnecessary columns
    house_df = house_df.drop([1702, 2180, 2181], axis=0)
    house_df = house_df.drop(["PID", "Order", "Mo Sold", "Sale Condition", "Sale Type", "Year Built", "Year Remod/Add"], axis=1)
    return house_df
```




```python
# The second function I created is used to select which featuures I'm using.
def select_features(house_df, coeff_threshold=0.4, uniq_threshold=10):
    #Drop columns with less than the correlation coefficient threshold
    numerical_df = house_df.select_dtypes(include=['int', 'float'])
    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()
    house_df = house_df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)
    
    nom_features = ["PID", "MS SubClass", "MS Zoning", "Street", "Alley", "Land Contour", "Lot Config", "Neighborhood", 
                    "Condition 1", "Condition 2", "Bldg Type", "House Style", "Roof Style", "Roof Matl", "Exterior 1st", 
                    "Exterior 2nd", "Mas Vnr Type", "Foundation", "Heating", "Central Air", "Garage Type", 
                    "Misc Feature", "Sale Type", "Sale Condition"]
    
    transform_cat_cols = []
    for col in nom_features:
        if col in house_df.columns:
            transform_cat_cols.append(col)

    unique_counts = house_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()
    drop_nonuniq_cols = unique_counts[unique_counts > 10].index
    house_df = house_df.drop(drop_nonuniq_cols, axis=1)
    
    text_cols = house_df.select_dtypes(include=['object'])
    for col in text_cols:
        house_df[col] = house_df[col].astype('category')
    house_df = pd.concat([house_df, pd.get_dummies(house_df.select_dtypes(include=['category']))], axis=1).drop(text_cols,axis=1)
        
    return house_df
```



```python
# Splitting training/test data, fitting training data on a regression model, and using either two different types of cross validation
def train_and_test(house_df, k=0):
    numeric_df = house_df.select_dtypes(include=['integer', 'float'])
    features = numeric_df.columns.drop("SalePrice")
    lr = linear_model.LinearRegression()

    ##---------------Cross Validation--------------------##     
    if k == 0:
        train = house_df[:1460]
        test = house_df[1460:]

        lr.fit(train[features], train["SalePrice"])
        predictions = lr.predict(test[features])
        mse = mean_squared_error(test["SalePrice"], predictions)
        rmse = np.sqrt(mse)

        return rmse, predictions
    
    if k == 1:
        # Randomize *all* rows (frac=1) from `house_df` and return
        shuffled_df = house_df.sample(frac=1, )
        train = house_df[:1460]
        test = house_df[1460:]
        
        lr.fit(train[features], train["SalePrice"])
        predictions_one = lr.predict(test[features])        
        
        mse_one = mean_squared_error(test["SalePrice"], predictions_one)
        rmse_one = np.sqrt(mse_one)
        
        lr.fit(test[features], test["SalePrice"])
        predictions_two = lr.predict(train[features])        
       
        mse_two = mean_squared_error(train["SalePrice"], predictions_two)
        rmse_two = np.sqrt(mse_two)
        
        avg_rmse = np.mean([rmse_one, rmse_two])
        print(rmse_one)
        print(rmse_two)
        return avg_rmse
    else:
        kf = KFold(n_splits=k, shuffle=True)
        rmse_values = []
        for train_index, test_index, in kf.split(house_df):
            train = house_df.iloc[train_index]
            test = house_df.iloc[test_index]
            lr.fit(train[features], train["SalePrice"])
            predictions = lr.predict(test[features])
            mse = mean_squared_error(test["SalePrice"], predictions)
            rmse = np.sqrt(mse)
            rmse_values.append(rmse)
        print(rmse_values)
        avg_rmse = np.mean(rmse_values)
        return avg_rmse
```




```python
# The output here are the predicted values of the 'SalePrice' column and measured errors (rmse/mse)
house_df = pd.read_csv("C:\\Users\\Matt\\Desktop\\AmesHousing.tsv", delimiter = "\t")
transform_df = trans_features(house_df)
filtered_df = select_features(transform_df)
rmse, predictions = train_and_test(filtered_df, k=0)

rmse
predictions
```

`
33367.28718340324
`

`
array([290692.96926164, 303594.66113516, 255613.50528603, ...,
124689.21970402, 192287.45741188, 249692.79300324])
`

I am testing the train_and_test function when k=0. The ouput for rmse(root mean square error) is 33367.29 and the second output are the values of the predicted prices. Note that the last function can be adjusted for any k. When k=0 holdout validation is used and when k>0 then k-fold cross validation using k-folds is used.